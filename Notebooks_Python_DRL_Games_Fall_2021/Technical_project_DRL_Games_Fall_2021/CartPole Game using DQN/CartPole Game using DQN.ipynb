{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "t81_558_class_12_03_keras_reinforce.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdZqDrmkH_fL"
      },
      "source": [
        "# CartPole Game using DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0WC-M08zNNq",
        "outputId": "ffa4e06a-a60d-49ba-ef5f-71c506aa4031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FreS1XOPzO9X",
        "outputId": "6a5e9117-316e-4e42-a63c-244e5cbdccfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if COLAB:\n",
        "  !sudo apt-get install -y xvfb ffmpeg x11-utils\n",
        "  #!pip install -q 'gym==0.10.11'\n",
        "  !pip install -q 'imageio==2.4.0'\n",
        "  !pip install -q PILLOW\n",
        "  #!pip install -q 'pyglet==1.3.2'\n",
        "  !pip install -q pyvirtualdisplay\n",
        "  !pip install -q tf-agents\n",
        "  #Added for gym for dependency fix and pyglet bug fix\n",
        "  #!pip install -q 'gym==0.21.0'\n",
        "  !pip install -q 'pyglet==1.5.21'\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires pyglet<=1.5.0,>=1.4.0, but you have pyglet 1.5.21 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "# Part 12.3: Keras Q-Learning in the OpenAI Gym\n",
        "\n",
        "Q-Learning, as we covered in the previous part, is a robust machine learning algorithm.  Unfortunately, Q-Learning requires that the Q-table contain an entry for every possible state that the environment can take.  If the environment only includes a handful of discrete state elements, then traditional Q-learning might be a good learning algorithm.  However, if the state space is large, the Q-table can become prohibitively large.\n",
        "\n",
        "Creating policies for large state spaces is a task that Deep Q-Learning Networks (DQN) can usually handle.  Unlike a table, a neural network does not require the program to represent every combination of state and action.  Neural networks can generalize these states and learn commonalities.  A DQN maps the state to its input neurons and the action Q-values to the output neurons.  The DQN effectively becomes a function that accepts state and suggestions an action by returning the expected reward for each of the possible actions. Figure 12.DQL demonstrates the DQN structure and mapping between state and action.\n",
        "\n",
        "**Figure 12.DQL: Deep Q-Learning (DQL)**\n",
        "![Deep Q-Learning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/deepqlearning.png \"Reinforcement Learning\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "As this diagram illustrates, the environment state contains several elements.  For the basic DQN the state can be a mix of continuous and categorical/discrete values.  For the DQN, the discrete state elements the program typically encoded as dummy variables. The actions should be discrete when your program implements a DQN.  Other algorithms support continuous outputs, which we will discuss later in this chapter.\n",
        "\n",
        "In this chapter, we will make use of [TF-Agents](https://www.tensorflow.org/agents) to implement a DQN to solve the cart-pole environment.  TF-Agents makes designing, implementing, and testing new RL algorithms easier by providing well tested modular components that can be modified and extended. It enables fast code iteration, with functional test integration and benchmarking.\n",
        "\n",
        "## DQN and the Cart-Pole Problem\n",
        "\n",
        "Barto (1983) first described the cart-pole problem. [[Cite:barto1983neuronlike]](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) A cart is connected to a hinged rigid pole.  The cart is free to move only in the vertical plane of the cart/track.  The agent can apply an implusive \"left\" or \"right\" force F of a fixed magnitude to the cart at discrete time intervals.  The cart-pole environment simulates the physics behind keeping the pole in a reasonably upright position on the cart.  The environment has four state variables:\n",
        "* $x$ The position of the cart on the track.\n",
        "* $\\theta$ The angle of the pole with the vertical\n",
        "* $\\dot{x}$ The cart velocity.\n",
        "* $\\dot{\\theta}$ The rate of change of the angle.\n",
        "\n",
        "The action space consists of discrete actions:\n",
        "* Apply force left\n",
        "* Apply force right\n",
        "\n",
        "To apply DQN to this problem, you need to create the following components for TF-Agents.\n",
        "\n",
        "* Environment\n",
        "* Agent\n",
        "* Policies\n",
        "* Metrics and Evaluation\n",
        "* Replay Buffer\n",
        "* Data Collection\n",
        "* Training\n",
        "\n",
        "These components are standard in most DQN implementations.  Later, we will apply these same components to an Atari game, and after that, a problem of our design. This example is based on the [cart-pole tutorial](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb) provided for TF-Agents.  We begin by importing needed Python libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMitx5qSgJk1"
      },
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6HsdS5GbSjd"
      },
      "source": [
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Several hyperparameters must be defined.  The TF-Agent example provided reasonably well-tuned hyperparameters for cart-pole.  Later we will adapt these to an Atari game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC1kNrOsLSIZ"
      },
      "source": [
        "# How long should training run?\n",
        "#num_iterations = 20000 \n",
        "# How many initial random steps, before training start, to\n",
        "# collect initial data.\n",
        "#initial_collect_steps = 1000   \n",
        "# How many steps should we run each iteration to collect \n",
        "# data from.\n",
        "#collect_steps_per_iteration = 1 \n",
        "# How much data should we store for training examples.\n",
        "#replay_buffer_max_length = 100000\n",
        "\n",
        "#batch_size = 64  \n",
        "#learning_rate = 1e-3 \n",
        "# How often should the program provide an update.\n",
        "##log_interval = 200  \n",
        "\n",
        "# How many episodes should the program use for each evaluation.\n",
        "#num_eval_episodes = 10\n",
        "# How often should an evaluation occur.\n",
        "#eval_interval = 1000  "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adjusting Parameters"
      ],
      "metadata": {
        "id": "6EgQFO2mpUZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How long should training run?\n",
        "num_iterations = 10000\n",
        "# How many initial random steps, before training start, to\n",
        "# collect initial data.\n",
        "initial_collect_steps = 500000   \n",
        "# How many steps should we run each iteration to collect \n",
        "# data from.\n",
        "collect_steps_per_iteration = 10\n",
        "# How much data should we store for training examples.\n",
        "replay_buffer_max_length = 1000000\n",
        "\n",
        "batch_size = 1024  \n",
        "learning_rate = 0.000001\n",
        "# How often should the program provide an update.\n",
        "log_interval = 200  \n",
        "\n",
        "# How many episodes should the program use for each evaluation.\n",
        "num_eval_episodes = 1\n",
        "# How often should an evaluation occur.\n",
        "eval_interval = 1000  "
      ],
      "metadata": {
        "id": "t6D3rFSWpXJn"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "TF-Agents uses OpenAI gym environments to represent the task or problem to be solved. Standard environments can be created in TF-Agents using **tf_agents.environments** suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control. We begin by loading the CartPole environment from the OpenAI Gym suite. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYEz-S9gEv2-"
      },
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "We will quickly render this environment to see the visual representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlO7WIQHu_7D",
        "outputId": "cf2514e2-ded5-4384-ed68-6a76ead91bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "#PIL.Image.fromarray(env.render())"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        ...,\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9_lskPOey18"
      },
      "source": [
        "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
        "\n",
        "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDv57iHfwQV",
        "outputId": "ea543eda-d3b6-440d-cbb0-eea280a8bf7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Spec:\n",
            "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxiSyCbBUQPi",
        "outputId": "50131802-b166-4520-9056-ac1f6524d681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward Spec:\n",
            "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttJ4uxZUQBr",
        "outputId": "3a747abd-1a59-4b30-cedd-9f0d4fe1083f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Spec:\n",
            "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "In the Cartpole environment:\n",
        "\n",
        "-   `observation` is an array of 4 floats: \n",
        "    -   the position and velocity of the cart\n",
        "    -   the angular position and velocity of the pole \n",
        "-   `reward` is a scalar float value\n",
        "-   `action` is a scalar integer with only two possible values:\n",
        "    -   `0` — \"move left\"\n",
        "    -   `1` — \"move right\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UGR5t_iZX-",
        "outputId": "1588d996-1b87-462c-da93-8946c5447c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time step:\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 0.00112028,  0.00560122, -0.04722315,  0.04080494], dtype=float32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'step_type': array(0, dtype=int32)})\n",
            "Next time step:\n",
            "TimeStep(\n",
            "{'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 0.00123231,  0.20136741, -0.04640705, -0.26639545], dtype=float32),\n",
            " 'reward': array(1., dtype=float32),\n",
            " 'step_type': array(1, dtype=int32)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Usually, the program instantiates two environments: one for training and one for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7brXNIGWXjC"
      },
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python and is converted to TF-Agents and TensorFlow using the **TFPyEnvironment** wrapper. The original environment's API uses Numpy arrays. The **TFPyEnvironment** turns these to **Tensors** to make it compatible with Tensorflow agents and policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-Y4mD6eDhF"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "An Agent represents the algorithm used to solve an RL problem. TF-Agents provides standard implementations of a variety of Agents:\n",
        "\n",
        "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this example)\n",
        "-   [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
        "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
        "\n",
        "You can only use the DQN agent in environments which have a discrete action space.  The DQN makes use of a QNetwork, a neural network model that learns to predict Q-Values (expected returns) for all actions, given a state from the environment.\n",
        "\n",
        "The following code uses **tf_agents.networks.q_network** to create a QNetwork, passing in the **observation_spec**, **action_spec**, and a tuple describing the number and size of the model's hidden layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgkdEPg_muzV"
      },
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Now we use **tf_agents.agents.dqn.dqn_agent** to instantiate a **DqnAgent**. In addition to the **time_step_spec**, **action_spec** and the QNetwork, the agent constructor also requires an optimizer (in this case, **AdamOptimizer**), a loss function, and an integer step counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY4yrjTEyc9"
      },
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
        "\n",
        "In this example:\n",
        "\n",
        "* The desired outcome is keeping the pole balanced upright over the cart.\n",
        "* The policy returns an action (left or right) for each `time_step` observation.\n",
        "\n",
        "Agents contain two policies: \n",
        "\n",
        "*  **agent.policy** -  The algorithm uses this main policy for evaluation and deployment.\n",
        "* **agent.collect_policy** - The algorithm this secondary policy for data collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwY7StuMkuV4"
      },
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Policies can be created independently of agents. For example, use **tf_agents.policies.random_tf_policy** to create a policy which will randomly select an action for each **time_step**. We will use this random policy to create initial collection data to begin training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE37-UCIrE69"
      },
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "To get an action from a policy, call the **policy.action(time_step)** method. The **time_step** contains the observation from the environment. This method returns a **PolicyStep**, which is a named tuple with three components:\n",
        "\n",
        "* **action** - The action to be taken (in this case, 0 or 1).\n",
        "* **state** - Used for stateful (that is, RNN-based) policies.\n",
        "* **info** - Auxiliary data, such as log probabilities of actions.\n",
        "\n",
        "Next we create an environment and setup the random policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gCcpXswVAxk",
        "outputId": "c40cfaa1-a15a-4d4c-9316-e6c1f6b59d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))\n",
        "time_step = example_environment.reset()\n",
        "random_policy.action(time_step)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return. The following function computes the average return of a policy, given the policy, environment, and a number of episodes.  We will use this same evaluation for Atari.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitzHo5_UbXy"
      },
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations \n",
        "# of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Running this computation on the `random_policy` shows a baseline performance in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bgU6Q6BZ8Bp",
        "outputId": "1cd54615-900b-445b-c72b-b28180a21e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62.0"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The replay buffer keeps track of data collected from the environment. This tutorial uses **TFUniformReplayBuffer**. The constructor requires the specs for the data it will be collecting. This value is available from the agent using the **collect_data_spec** method. The batch size and maximum buffer length are also required.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX2zGUWJGWAl"
      },
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "For most agents, **collect_data_spec** is a named tuple called **Trajectory**, containing the specs for observations, actions, rewards, and other items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZ-3HcqgE1z",
        "outputId": "23a2a4c5-f39a-4afd-d7f1-3a3feb5501c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "agent.collect_data_spec"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Trajectory(\n",
              "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
              " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
              " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
              " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
              "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
              "      dtype=float32)),\n",
              " 'policy_info': (),\n",
              " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
              " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1KSAEGG4h9"
      },
      "source": [
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "The replay buffer is now a collection of Trajectories. The agent needs access to the replay buffer. TF-Agents provides this access by creating an iterable **tf.data.Dataset** pipeline, which will feed data to the agent.\n",
        "\n",
        "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (**num_steps=2**).\n",
        "\n",
        "The program also optimizes this dataset by running parallel calls and prefetching data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7bilizt_qW",
        "outputId": "cbeb8ad2-5928-4d58-b0e8-1ead1b9c99f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "\n",
        "dataset"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: (Trajectory(\n",
              "{action: (1024, 2),\n",
              " discount: (1024, 2),\n",
              " next_step_type: (1024, 2),\n",
              " observation: (1024, 2, 4),\n",
              " policy_info: (),\n",
              " reward: (1024, 2),\n",
              " step_type: (1024, 2)}), BufferInfo(ids=(1024, 2), probabilities=(1024,))), types: (Trajectory(\n",
              "{action: tf.int64,\n",
              " discount: tf.float32,\n",
              " next_step_type: tf.int32,\n",
              " observation: tf.float32,\n",
              " policy_info: (),\n",
              " reward: tf.float32,\n",
              " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K13AST-2ppOq",
        "outputId": "994948b4-f3c5-4404-96b5-4a7d1379c90e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3d5e82bfd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "Two things must happen during the training loop:\n",
        "\n",
        "* Collect data from the environment\n",
        "* Use that data to train the agent's neural network(s)\n",
        "\n",
        "This example also periodically evaluates the policy and prints the current score.\n",
        "\n",
        "The following will take ~5 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pTbJ3PeyF-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587fef00-75e8-4111-cff0-75b002da7333"
      },
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph \n",
        "\n",
        "# start time\n",
        "import time\n",
        "start = time.time()\n",
        "# using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, \\\n",
        "                                num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, \\\n",
        "                                    num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "print('Time taken to train model: ', time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 200: loss = 9.528324127197266\n",
            "step = 400: loss = 15.653562545776367\n",
            "step = 600: loss = 26.211393356323242\n",
            "step = 800: loss = 25.279029846191406\n",
            "step = 1000: loss = 30.416976928710938\n",
            "step = 1000: Average Return = 182.0\n",
            "step = 1200: loss = 37.79280471801758\n",
            "step = 1400: loss = 39.55956268310547\n",
            "step = 1600: loss = 68.94821166992188\n",
            "step = 1800: loss = 62.29082489013672\n",
            "step = 2000: loss = 63.277076721191406\n",
            "step = 2000: Average Return = 200.0\n",
            "step = 2200: loss = 187.65219116210938\n",
            "step = 2400: loss = 136.1652069091797\n",
            "step = 2600: loss = 262.06536865234375\n",
            "step = 2800: loss = 290.9677734375\n",
            "step = 3000: loss = 206.5137939453125\n",
            "step = 3000: Average Return = 200.0\n",
            "step = 3200: loss = 163.912841796875\n",
            "step = 3400: loss = 156.84580993652344\n",
            "step = 3600: loss = 582.6834106445312\n",
            "step = 3800: loss = 135.34869384765625\n",
            "step = 4000: loss = 197.08151245117188\n",
            "step = 4000: Average Return = 200.0\n",
            "step = 4200: loss = 181.1516571044922\n",
            "step = 4400: loss = 115.57750701904297\n",
            "step = 4600: loss = 278.1878662109375\n",
            "step = 4800: loss = 254.1443634033203\n",
            "step = 5000: loss = 328.27703857421875\n",
            "step = 5000: Average Return = 200.0\n",
            "step = 5200: loss = 561.548095703125\n",
            "step = 5400: loss = 304.38385009765625\n",
            "step = 5600: loss = 366.42510986328125\n",
            "step = 5800: loss = 341.64080810546875\n",
            "step = 6000: loss = 173.71499633789062\n",
            "step = 6000: Average Return = 200.0\n",
            "step = 6200: loss = 378.5887451171875\n",
            "step = 6400: loss = 434.015869140625\n",
            "step = 6600: loss = 359.92193603515625\n",
            "step = 6800: loss = 639.6151123046875\n",
            "step = 7000: loss = 479.6058349609375\n",
            "step = 7000: Average Return = 200.0\n",
            "step = 7200: loss = 750.047607421875\n",
            "step = 7400: loss = 354.129150390625\n",
            "step = 7600: loss = 496.9447021484375\n",
            "step = 7800: loss = 586.691162109375\n",
            "step = 8000: loss = 1143.0164794921875\n",
            "step = 8000: Average Return = 200.0\n",
            "step = 8200: loss = 1120.85498046875\n",
            "step = 8400: loss = 402.5688171386719\n",
            "step = 8600: loss = 766.08154296875\n",
            "step = 8800: loss = 1037.685791015625\n",
            "step = 9000: loss = 979.0481567382812\n",
            "step = 9000: Average Return = 200.0\n",
            "step = 9200: loss = 1291.6361083984375\n",
            "step = 9400: loss = 1167.0555419921875\n",
            "step = 9600: loss = 1018.4673461914062\n",
            "step = 9800: loss = 1756.7764892578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "Use **matplotlib.pyplot** to chart how the policy improved during training.\n",
        "\n",
        "One iteration of **Cartpole-v0** consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts show the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase each time monotonically.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtL1mbOYCVO"
      },
      "source": [
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
        "\n",
        "First, create a function to embed videos in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULaGr8pvOKbl"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owOVWB158NlF"
      },
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povaAOcZygLw"
      },
      "source": [
        "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZIdC37yNH4"
      },
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w1D-hbSTUywt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
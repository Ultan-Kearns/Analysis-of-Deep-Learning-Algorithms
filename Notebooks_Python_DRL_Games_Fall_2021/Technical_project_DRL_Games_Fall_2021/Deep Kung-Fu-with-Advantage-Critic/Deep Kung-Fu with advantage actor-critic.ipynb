{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ultan-Kearns/Artificial-Intelligence-CA/blob/master/Notebooks_Python_DRL_Games_Fall_2021/Technical_project_DRL_Games_Fall_2021/Deep%20Kung-Fu-with-Advantage-Critic/Deep%20Kung-Fu%20with%20advantage%20actor-critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjC4uqSP17ai"
      },
      "source": [
        "# Deep Kung-Fu with advantage actor-critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRXcuuh3qB6q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdlrlePjq-g",
        "outputId": "0d32ee9f-213c-444c-f36d-6b85ccfd9481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcOgo4xwj4PJ",
        "outputId": "be950ce4-7828-4f8c-e300-b9dca964a801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install python-opengl -y"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aS0God8kBDE",
        "outputId": "e0987fe8-9c6c-4a32-f2e8-8445ba97bc3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt install xvfb -y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZuEBujtkKoj",
        "outputId": "da70cab4-d883-42f0-878f-142418787a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (1.1)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.7/dist-packages (from piglet) (1.2.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.4.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.7)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfq6FvJ9kW1G",
        "outputId": "100c21c9-c447-4af2-c0b7-34c880c0452d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f4b9f1dad90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW0KjeOVkc88"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from IPython.core import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "#If you are running on a server, launch xvfb to record game videos\n",
        "#Please make sure you have xvfb installed\n",
        "import os\n",
        "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsbpF4In-3c"
      },
      "source": [
        "# Atari Environment\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w1HpxbzX81j",
        "outputId": "ff0df471-7fdc-4702-f887-f38af0728b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install scipy==1.1.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR12anpKlcId",
        "outputId": "fef8b6dd-1cfe-4bda-adf8-a2844f04d68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Package 'libav-tools' has no installation candidate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl5urImpnSBQ",
        "outputId": "0bdc1e86-20b0-4261-d79f-d653db870fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m pip install \"gym[atari,accept-rom-license]\"\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'accept-rom-license'\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[accept-rom-license,atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[accept-rom-license,atari]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount gdrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Atari depreacated installing ale-py instead\n",
        "!python -m pip install ale-py\n",
        "!pip install gym[atari] \n",
        "from ale_py import ALEInterface \n",
        "ale = ALEInterface()\n",
        "\n",
        "!ale-import-roms '/content/gdrive/My Drive/roms'\n",
        "\n",
        "ale.loadROM('KungFuMaster')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSxKeJXPYXjC",
        "outputId": "73d46af8-5ee0-42c9-e8f8-e2c0197e365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py) (5.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ale-py) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.10.0.2)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout /content/gdrive/My Drive/roms/Breakout.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master /content/gdrive/My Drive/roms/KungFuMaster.bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 2 / 2 ROMs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Had issue with preprocess atari so needed code from Github\n",
        "# Reference: https://github.com/sshkhr/Practical_RL/blob/master/week6_policy_based/atari_util.py\n",
        "# Code by https://github.com/sshkhr \n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Auxilary files for those who wanted to solve breakout with CEM or policy gradient\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from scipy.misc import imresize\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "\n",
        "class PreprocessAtari(Wrapper):\n",
        "    def __init__(self, env, height=42, width=42, color=False, crop=lambda img: img, \n",
        "                 n_frames=4, dim_order='theano', reward_scale=1,):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(PreprocessAtari, self).__init__(env)\n",
        "        assert dim_order in ('theano', 'tensorflow')\n",
        "        self.img_size = (height, width)\n",
        "        self.crop=crop\n",
        "        self.color=color\n",
        "        self.dim_order = dim_order\n",
        "        self.reward_scale = reward_scale\n",
        "        \n",
        "        n_channels = (3 * n_frames) if color else n_frames\n",
        "        obs_shape = [n_channels,height,width] if dim_order == 'theano' else [height,width,n_channels]\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def step(self,action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward * self.reward_scale, done, info\n",
        "    \n",
        "    ### image processing ###\n",
        "    \n",
        "    def update_buffer(self,img):\n",
        "        img = self.preproc_image(img)\n",
        "        offset = 3 if self.color else 1\n",
        "        if self.dim_order == 'theano':\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        else:\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
        "\n",
        "    def preproc_image(self, img):\n",
        "        \"\"\"what happens to the observation\"\"\"\n",
        "        img = self.crop(img)\n",
        "        img = imresize(img, self.img_size)\n",
        "        if not self.color:\n",
        "            img = img.mean(-1, keepdims=True)\n",
        "        if self.dim_order == 'theano':\n",
        "            img = img.transpose([2,0,1]) # [h, w, c] to [c, h, w]\n",
        "        img = img.astype('float32') / 255.\n",
        "        return img"
      ],
      "metadata": {
        "id": "fCZdVhCN5lKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMEwvBMNoRdt"
      },
      "source": [
        "# Deep Kung-Fu with advantage actor-critic\n",
        "\n",
        "---\n",
        "\n",
        "For starters, let's take a look at the game itself:\n",
        "\n",
        " * Image resized to 42x42 and grayscale to run faster\n",
        " * Rewards divided by 100 'cuz they are all divisible by 100\n",
        " * Agent sees last 4 frames of game to account for object velocity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRSoMY-Tkkmb"
      },
      "source": [
        "import gym\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"KungFuMaster-v0\")\n",
        "    env = PreprocessAtari(env, height=42, width=42,\n",
        "                          crop = lambda img: img[60:-30, 5:],\n",
        "                          dim_order = 'tensorflow',\n",
        "                          color=False, n_frames=4,\n",
        "                          reward_scale = 0.01)\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "obs_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "print(\"Num actions:\", n_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ems5qHMeZU0q"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pd1KapnM0A"
      },
      "source": [
        "s = env.reset()\n",
        "for _ in range(100):\n",
        "    s, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.title('Game image')\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "plt.show()\n",
        "\n",
        "plt.title('Agent observation (4-frame buffer)')\n",
        "plt.imshow(s.transpose([0,2,1]).reshape([42,-1]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdlsSzJovVP"
      },
      "source": [
        "# Build an agent\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We now have to build an agent for actor-critic training - a convolutional neural network that converts states into action probabilities and state values .\n",
        "\n",
        "Your assignment here is to build and apply a neural network - with any framework you want.\n",
        "\n",
        "For starters, we want you to implement this architecture: \n",
        "\n",
        "After you get above 50 points, we encourage you to experiment with model architecture to score even better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Qg7FRfkokL"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "sess = tf.compat.v1.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRb-25lKoAjy"
      },
      "source": [
        "from keras.layers import Conv2D, Dense, Flatten, Input\n",
        "import keras\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
        "        \"\"\"A simple actor-critic agent\"\"\"\n",
        "        \n",
        "        with tf.compat.v1.variable_scope(name, reuse=reuse):\n",
        "            ####\n",
        "            inputs = Input(shape=state_shape)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(inputs)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "            \n",
        "            # two different output layers\n",
        "            logits = Dense(n_actions, activation='linear')(x)\n",
        "            state_value = Dense(1, activation='linear')(x)\n",
        "            \n",
        "            self.network = Model(inputs=inputs, outputs=[logits, state_value])\n",
        "            \n",
        "            \n",
        "            # prepare a graph for agent step\n",
        "            self.state_t = tf.compat.v1.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
        "        \n",
        "    def symbolic_step(self, state_t):\n",
        "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
        "        \n",
        "        # Apply neural network\n",
        "        ### Apply agent's neural network to get policy logits and state values.\n",
        "#         network_output = self.network(state_t)\n",
        "\n",
        "#         logits = network_output[:,1:]\n",
        "#         state_value = network_output[:,0]\n",
        "        \n",
        "        \n",
        "        logits, state_value = self.network(state_t)\n",
        "        state_value = state_value[:, 0]\n",
        "        \n",
        "        assert tf.compat.v1.is_numeric_tensor(state_value) and state_value.shape.ndims == 1, \\\n",
        "            \"please return 1D tf tensor of state values [you got %s]\" % repr(state_value)\n",
        "        assert tf.compat.v1.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
        "            \"please return 2d tf tensor of logits [you got %s]\" % repr(logits)\n",
        "        # if you triggered state_values assert with your shape being [None, 1], \n",
        "        # just select [:, 0]-th element of state values as new state values\n",
        "        \n",
        "        return (logits, state_value)\n",
        "    \n",
        "    def step(self, state_t):\n",
        "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
        "        sess = tf.compat.v1.get_default_session()\n",
        "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
        "    \n",
        "    def sample_actions(self, agent_outputs):\n",
        "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
        "        logits, state_values = agent_outputs\n",
        "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
        "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating hyper parameters\n"
      ],
      "metadata": {
        "id": "pwSNyBPU5WYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, Dense, Flatten, Input\n",
        "import keras\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
        "        \"\"\"A simple actor-critic agent\"\"\"\n",
        "        \n",
        "        with tf.compat.v1.variable_scope(name, reuse=reuse):\n",
        "            ####\n",
        "            inputs = Input(shape=state_shape)\n",
        "            x = Conv2D(512, (3, 3), strides=1, activation='relu')(inputs)\n",
        "            x = Conv2D(512, (3, 3), strides=1, activation='relu')(x)\n",
        "            x = Conv2D(512, (3, 3), strides=1, activation='relu')(x)\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(512, activation='softmax')(x)\n",
        "            \n",
        "            # two different output layers\n",
        "            logits = Dense(n_actions, activation='linear')(x)\n",
        "            state_value = Dense(1, activation='linear')(x)\n",
        "            \n",
        "            self.network = Model(inputs=inputs, outputs=[logits, state_value])\n",
        "            \n",
        "            \n",
        "            # prepare a graph for agent step\n",
        "            self.state_t = tf.compat.v1.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
        "        \n",
        "    def symbolic_step(self, state_t):\n",
        "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
        "        \n",
        "        # Apply neural network\n",
        "        ### Apply agent's neural network to get policy logits and state values.\n",
        "#         network_output = self.network(state_t)\n",
        "\n",
        "#         logits = network_output[:,1:]\n",
        "#         state_value = network_output[:,0]\n",
        "        \n",
        "        \n",
        "        logits, state_value = self.network(state_t)\n",
        "        state_value = state_value[:, 0]\n",
        "        \n",
        "        assert tf.compat.v1.is_numeric_tensor(state_value) and state_value.shape.ndims == 1, \\\n",
        "            \"please return 1D tf tensor of state values [you got %s]\" % repr(state_value)\n",
        "        assert tf.compat.v1.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
        "            \"please return 2d tf tensor of logits [you got %s]\" % repr(logits)\n",
        "        # if you triggered state_values assert with your shape being [None, 1], \n",
        "        # just select [:, 0]-th element of state values as new state values\n",
        "        \n",
        "        return (logits, state_value)\n",
        "    \n",
        "    def step(self, state_t):\n",
        "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
        "        sess = tf.compat.v1.get_default_session()\n",
        "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
        "    \n",
        "    def sample_actions(self, agent_outputs):\n",
        "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
        "        logits, state_values = agent_outputs\n",
        "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
        "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
      ],
      "metadata": {
        "id": "v9wsyTNc5Z7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjtQetyEoHMD"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "agent = Agent(\"agent\", obs_shape, n_actions)\n",
        "sess.run(tf.compat.v1.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUs5NBuoNWB"
      },
      "source": [
        "state = [env.reset()]\n",
        "logits, value = agent.step(state)\n",
        "print(\"action logits:\\n\", logits)\n",
        "print(\"state values:\\n\", value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqIoeqmo4n5"
      },
      "source": [
        "# Let's play!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's build a function that measures agent's average reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AFcqPHKoQ_i"
      },
      "source": [
        "def evaluate(agent, env, n_games=1):\n",
        "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
        "\n",
        "    game_rewards = []\n",
        "    for _ in range(n_games):\n",
        "        state = env.reset()\n",
        "        \n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.sample_actions(agent.step([state]))[0]\n",
        "            state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done: break\n",
        "                \n",
        "        game_rewards.append(total_reward)\n",
        "    return game_rewards\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUPl5-sKoVpg"
      },
      "source": [
        "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
        "rw = evaluate(agent, env_monitor, n_games=3,)\n",
        "env_monitor.close()\n",
        "print (rw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcr9OW2IoZU1"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "# needed to import code to fix issue with video also changed the dir to remove \n",
        "# underscore\n",
        "# ref for this code below https://stackoverflow.com/questions/57377185/how-play-mp4-video-in-google-colab\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"./kungfu_videos/\"+video_names[0],'rb').read()\n",
        "agent_video = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=%s type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" %agent_video) #this may or may not be _last_ video. Try other indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmDLcuUpGMj"
      },
      "source": [
        "# Training on parallel games\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/f894eba2ee745652f1fc95484cfa45bf26decb8e/68747470733a2f2f6769746875622e636f6d2f79616e646578646174617363686f6f6c2f50726163746963616c5f524c2f7261772f6d61737465722f7965745f616e6f746865725f7765656b2f5f7265736f757263652f2f656e765f706f6f6c2e706e67)\n",
        "\n",
        "To make actor-critic training more stable, we shall play several games in parallel. This means ya'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for ya - just make sure you read it before you use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX-uaGwCojPt"
      },
      "source": [
        "class EnvBatch:\n",
        "    def __init__(self, n_envs = 10):\n",
        "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
        "        self.envs = [make_env() for _ in range(n_envs)]\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
        "        return np.array([env.reset() for env in self.envs])\n",
        "    \n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Send a vector[batch_size] of actions into respective environments\n",
        "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
        "        \"\"\"\n",
        "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
        "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
        "        \n",
        "        # reset environments automatically\n",
        "        for i in range(len(self.envs)):\n",
        "            if done[i]:\n",
        "                new_obs[i] = self.envs[i].reset()\n",
        "        \n",
        "        return new_obs, rewards, done, infos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JawlpBFo0Yb"
      },
      "source": [
        "env_batch = EnvBatch(10)\n",
        "\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
        "\n",
        "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "\n",
        "print(\"State shape:\", batch_states.shape)\n",
        "print(\"Actions:\", batch_actions[:3])\n",
        "print(\"Rewards:\", batch_rewards[:3])\n",
        "print(\"Done:\", batch_done[:3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwEPCL46pfHO"
      },
      "source": [
        "# Actor-critic\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we define a loss functions and learning algorithms as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C6zD_V1o35d"
      },
      "source": [
        "# These placeholders mean exactly the same as in \"Let's try it out\" section above\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "states_ph = tf.compat.v1.placeholder('float32', [None,] + list(obs_shape))    \n",
        "next_states_ph =  tf.compat.v1.placeholder('float32', [None,] + list(obs_shape))\n",
        "actions_ph =  tf.compat.v1.placeholder('int32', (None,))\n",
        "rewards_ph =  tf.compat.v1.placeholder('float32', (None,))\n",
        "is_done_ph =  tf.compat.v1.placeholder('float32', (None,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktpq6pJgo-Dl"
      },
      "source": [
        "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
        "logits, state_values = agent.symbolic_step(states_ph)\n",
        "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
        "next_state_values = next_state_values * (1 - is_done_ph)\n",
        "\n",
        "# probabilities and log-probabilities for all actions\n",
        "probs = tf.nn.softmax(logits)            # [n_envs, n_actions]\n",
        "logprobs = tf.nn.log_softmax(logits)     # [n_envs, n_actions]\n",
        "\n",
        "# log-probabilities only for agent's chosen actions\n",
        "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aNY8wRpBk9"
      },
      "source": [
        "\n",
        "\n",
        "# compute advantage using rewards_ph, state_values and next_state_values\n",
        "gamma = 0.99\n",
        "advantage = rewards_ph + gamma*next_state_values - state_values\n",
        "\n",
        "assert advantage.shape.ndims == 1, \"please compute advantage for each sample, vector of shape [n_envs,]\"\n",
        "\n",
        "# compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
        "entropy =  -tf.reduce_sum(probs * logprobs, 1, name=\"entropy\")\n",
        "\n",
        "assert entropy.shape.ndims == 1, \"please compute pointwise entropy vector of shape [n_envs,] \"\n",
        "\n",
        "\n",
        "\n",
        "actor_loss =  - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage)) - 0.001 * tf.reduce_mean(entropy)\n",
        "\n",
        "# compute target state values using temporal difference formula. Use rewards_ph and next_step_values\n",
        "target_state_values = rewards_ph+gamma*next_state_values\n",
        "\n",
        "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2 )\n",
        "\n",
        "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
        "sess.run(tf.compat.v1.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9qCcr3cpEtw"
      },
      "source": [
        "# Sanity checks to catch some errors. Specific to KungFuMaster in assignment's default setup.\n",
        "l_act, l_crit, adv, ent = sess.run([actor_loss, critic_loss, advantage, entropy], feed_dict = {\n",
        "        states_ph: batch_states,\n",
        "        actions_ph: batch_actions,\n",
        "        next_states_ph: batch_states,\n",
        "        rewards_ph: batch_rewards,\n",
        "        is_done_ph: batch_done,\n",
        "    })\n",
        "\n",
        "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
        "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
        "if ent.mean() < np.log(n_actions) / 2: print(\"Entropy is too low for untrained agent\")\n",
        "print(\"You just might be fine!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGMW7v42peBa"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFWsGKlpuIM"
      },
      "source": [
        "# Train\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/69cc1e7cffa2ebd5db67c39f7c7c2724aaf5594b/687474703a2f2f696d61676573362e66616e706f702e636f6d2f696d6167652f70686f746f732f33383930303030302f44616e69656c2d73616e2d747261696e696e672d7468652d6b61726174652d6b69642d33383934373336312d3439392d3238382e676966)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6TQ0p8ul1ZA"
      },
      "source": [
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpqugPcIpIlq"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "#from pandas import ewm\n",
        "\n",
        "env_batch = EnvBatch(10)\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "rewards_history = []\n",
        "entropy_history = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-uNJ4-TpNC_"
      },
      "source": [
        "for i in trange(100000): \n",
        "    \n",
        "    batch_actions = agent.sample_actions(agent.step(batch_states))\n",
        "    batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "    \n",
        "    feed_dict = {\n",
        "        states_ph: batch_states,\n",
        "        actions_ph: batch_actions,\n",
        "        next_states_ph: batch_next_states,\n",
        "        rewards_ph: batch_rewards,\n",
        "        is_done_ph: batch_done,\n",
        "    }\n",
        "    batch_states = batch_next_states\n",
        "    \n",
        "    _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
        "    entropy_history.append(np.mean(ent_t))\n",
        "\n",
        "    if i % 500 == 0: \n",
        "        if i % 2500 == 0:\n",
        "            rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
        "            if rewards_history[-1] >= 50:\n",
        "                print(\"Your agent has earned the yellow belt\")\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=[8,4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(rewards_history, label='rewards')\n",
        "        df = pd.DataFrame(rewards_history)\n",
        "        plt.plot(df.ewm(halflife=1000).mean(), marker='.', label='rewards ewma@10')\n",
        "        plt.title(\"Session rewards\"); plt.grid(); plt.legend()\n",
        "        \n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(entropy_history, label='entropy')\n",
        "        df2 = pd.DataFrame(entropy_history)\n",
        "        plt.plot(df2.ewm(halflife=1000).mean(), label='entropy ewma@1000')\n",
        "        plt.title(\"Policy entropy\"); plt.grid(); plt.legend()        \n",
        "        plt.show()\n",
        "        \n",
        "df = pd.DataFrame(data=batch_rewards)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC3JUqqPpo6k"
      },
      "source": [
        "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
        "final_rewards = evaluate(agent, env_monitor, n_games=1,)\n",
        "env_monitor.close()\n",
        "print(\"Final mean reward:\", np.mean(final_rewards))\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwb5QCk3YUb"
      },
      "source": [
        "df.ewm(halflife=12).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jghRDEjNThIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
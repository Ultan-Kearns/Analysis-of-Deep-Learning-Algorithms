{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjC4uqSP17ai"
      },
      "source": [
        "# Deep Kung-Fu with advantage actor-critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRXcuuh3qB6q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWdlrlePjq-g"
      },
      "source": [
        "!pip install 'gym 0.19.0' -e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcOgo4xwj4PJ"
      },
      "source": [
        "!apt-get install python-opengl -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aS0God8kBDE"
      },
      "source": [
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZuEBujtkKoj"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfq6FvJ9kW1G"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW0KjeOVkc88"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from IPython.core import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "#If you are running on a server, launch xvfb to record game videos\n",
        "#Please make sure you have xvfb installed\n",
        "import os\n",
        "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsbpF4In-3c"
      },
      "source": [
        "# Atari Environment\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w1HpxbzX81j"
      },
      "source": [
        "pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJu7AeaZk7Dn"
      },
      "source": [
        "\"\"\"Auxilary files for those who wanted to solve breakout with CEM or policy gradient\"\"\"\n",
        "import numpy as np\n",
        "import gym\n",
        "from scipy.misc import imresize\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "\n",
        "class PreprocessAtari(Wrapper):\n",
        "    def __init__(self, env, height=42, width=42, color=False, crop=lambda img: img, \n",
        "                 n_frames=4, dim_order='theano', reward_scale=1,):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(PreprocessAtari, self).__init__(env)\n",
        "        assert dim_order in ('theano', 'tensorflow')\n",
        "        self.img_size = (height, width)\n",
        "        self.crop=crop\n",
        "        self.color=color\n",
        "        self.dim_order = dim_order\n",
        "        self.reward_scale = reward_scale\n",
        "        \n",
        "        n_channels = (3 * n_frames) if color else n_frames\n",
        "        obs_shape = [n_channels,height,width] if dim_order == 'theano' else [height,width,n_channels]\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def step(self,action):\n",
        "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward * self.reward_scale, done, info\n",
        "    \n",
        "    ### image processing ###\n",
        "    \n",
        "    def update_buffer(self,img):\n",
        "        img = self.preproc_image(img)\n",
        "        offset = 3 if self.color else 1\n",
        "        if self.dim_order == 'theano':\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        else:\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
        "\n",
        "    def preproc_image(self, img):\n",
        "        \"\"\"what happens to the observation\"\"\"\n",
        "        img = self.crop(img)\n",
        "        img = imresize(img, self.img_size)\n",
        "        if not self.color:\n",
        "            img = img.mean(-1, keepdims=True)\n",
        "        if self.dim_order == 'theano':\n",
        "            img = img.transpose([2,0,1]) # [h, w, c] to [c, h, w]\n",
        "        img = img.astype('float32') / 255.\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR12anpKlcId"
      },
      "source": [
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl5urImpnSBQ"
      },
      "source": [
        "!python -m pip install \"gym[atari,accept-rom-license]\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount gdrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Atari depreacated installing ale-py instead\n",
        "!python -m pip install ale-py\n",
        "!pip uninstall gym atari-py ale-py\n",
        "!pip install gym[atari] \n",
        "from ale_py import ALEInterface \n",
        "ale = ALEInterface()\n",
        "\n",
        "!ale-import-roms '/content/gdrive/My Drive/roms'\n",
        "\n",
        "from ale_py.roms import KungFuMaster\n",
        "\n",
        "ale.loadROM('KungFuMaster')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSxKeJXPYXjC",
        "outputId": "1b365ac7-a251-4723-a3aa-f3e064eed1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py) (5.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from ale-py) (4.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->ale-py) (3.7.0)\n",
            "Found existing installation: gym 0.19.0\n",
            "Uninstalling gym-0.19.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/gym-0.19.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/gym/envs/atari/environment.py\n",
            "Proceed (y/n)? "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMEwvBMNoRdt"
      },
      "source": [
        "# Deep Kung-Fu with advantage actor-critic\n",
        "\n",
        "---\n",
        "\n",
        "For starters, let's take a look at the game itself:\n",
        "\n",
        " * Image resized to 42x42 and grayscale to run faster\n",
        " * Rewards divided by 100 'cuz they are all divisible by 100\n",
        " * Agent sees last 4 frames of game to account for object velocity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRSoMY-Tkkmb"
      },
      "source": [
        "import gym\n",
        "import retro\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"KungFuMaster-v0\")\n",
        "    env = PreprocessAtari(env, height=42, width=42,\n",
        "                          crop = lambda img: img[60:-30, 5:],\n",
        "                          dim_order = 'tensorflow',\n",
        "                          color=False, n_frames=4,\n",
        "                          reward_scale = 0.01)\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "obs_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "print(\"Num actions:\", n_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ems5qHMeZU0q"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pd1KapnM0A"
      },
      "source": [
        "s = env.reset()\n",
        "for _ in range(100):\n",
        "    s, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.title('Game image')\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "plt.show()\n",
        "\n",
        "plt.title('Agent observation (4-frame buffer)')\n",
        "plt.imshow(s.transpose([0,2,1]).reshape([42,-1]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdlsSzJovVP"
      },
      "source": [
        "# Build an agent\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We now have to build an agent for actor-critic training - a convolutional neural network that converts states into action probabilities and state values .\n",
        "\n",
        "Your assignment here is to build and apply a neural network - with any framework you want.\n",
        "\n",
        "For starters, we want you to implement this architecture: \n",
        "\n",
        "After you get above 50 points, we encourage you to experiment with model architecture to score even better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0Qg7FRfkokL"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.reset_default_graph()\n",
        "sess = tf.compat.v1.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRb-25lKoAjy"
      },
      "source": [
        "from keras.layers import Conv2D, Dense, Flatten, Input\n",
        "import keras\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
        "        \"\"\"A simple actor-critic agent\"\"\"\n",
        "        \n",
        "        with tf.compat.v1.variable_scope(name, reuse=reuse):\n",
        "            ####\n",
        "            inputs = Input(shape=state_shape)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(inputs)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x = Flatten()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "            \n",
        "            # two different output layers\n",
        "            logits = Dense(n_actions, activation='linear')(x)\n",
        "            state_value = Dense(1, activation='linear')(x)\n",
        "            \n",
        "            self.network = Model(inputs=inputs, outputs=[logits, state_value])\n",
        "            \n",
        "            \n",
        "            # prepare a graph for agent step\n",
        "            self.state_t = tf.compat.v1.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
        "        \n",
        "    def symbolic_step(self, state_t):\n",
        "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
        "        \n",
        "        # Apply neural network\n",
        "        ### Apply agent's neural network to get policy logits and state values.\n",
        "#         network_output = self.network(state_t)\n",
        "\n",
        "#         logits = network_output[:,1:]\n",
        "#         state_value = network_output[:,0]\n",
        "        \n",
        "        \n",
        "        logits, state_value = self.network(state_t)\n",
        "        state_value = state_value[:, 0]\n",
        "        \n",
        "        assert tf.compat.v1.is_numeric_tensor(state_value) and state_value.shape.ndims == 1, \\\n",
        "            \"please return 1D tf tensor of state values [you got %s]\" % repr(state_value)\n",
        "        assert tf.compat.v1.is_numeric_tensor(logits) and logits.shape.ndims == 2, \\\n",
        "            \"please return 2d tf tensor of logits [you got %s]\" % repr(logits)\n",
        "        # if you triggered state_values assert with your shape being [None, 1], \n",
        "        # just select [:, 0]-th element of state values as new state values\n",
        "        \n",
        "        return (logits, state_value)\n",
        "    \n",
        "    def step(self, state_t):\n",
        "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
        "        sess = tf.compat.v1.get_default_session()\n",
        "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
        "    \n",
        "    def sample_actions(self, agent_outputs):\n",
        "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
        "        logits, state_values = agent_outputs\n",
        "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
        "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjtQetyEoHMD"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "agent = Agent(\"agent\", obs_shape, n_actions)\n",
        "sess.run(tf.compat.v1.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUs5NBuoNWB"
      },
      "source": [
        "state = [env.reset()]\n",
        "logits, value = agent.step(state)\n",
        "print(\"action logits:\\n\", logits)\n",
        "print(\"state values:\\n\", value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqIoeqmo4n5"
      },
      "source": [
        "# Let's play!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's build a function that measures agent's average reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AFcqPHKoQ_i"
      },
      "source": [
        "def evaluate(agent, env, n_games=1):\n",
        "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
        "\n",
        "    game_rewards = []\n",
        "    for _ in range(n_games):\n",
        "        state = env.reset()\n",
        "        \n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.sample_actions(agent.step([state]))[0]\n",
        "            state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done: break\n",
        "                \n",
        "        game_rewards.append(total_reward)\n",
        "    return game_rewards\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUPl5-sKoVpg"
      },
      "source": [
        "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
        "rw = evaluate(agent, env_monitor, n_games=3,)\n",
        "env_monitor.close()\n",
        "print (rw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcr9OW2IoZU1"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./kungfu_videos/\"+video_names[1])) #this may or may not be _last_ video. Try other indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmDLcuUpGMj"
      },
      "source": [
        "# Training on parallel games\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/f894eba2ee745652f1fc95484cfa45bf26decb8e/68747470733a2f2f6769746875622e636f6d2f79616e646578646174617363686f6f6c2f50726163746963616c5f524c2f7261772f6d61737465722f7965745f616e6f746865725f7765656b2f5f7265736f757263652f2f656e765f706f6f6c2e706e67)\n",
        "\n",
        "To make actor-critic training more stable, we shall play several games in parallel. This means ya'll have to initialize several parallel gym envs, send agent's actions there and .reset() each env if it becomes terminated. To minimize learner brain damage, we've taken care of them for ya - just make sure you read it before you use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX-uaGwCojPt"
      },
      "source": [
        "class EnvBatch:\n",
        "    def __init__(self, n_envs = 10):\n",
        "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
        "        self.envs = [make_env() for _ in range(n_envs)]\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
        "        return np.array([env.reset() for env in self.envs])\n",
        "    \n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Send a vector[batch_size] of actions into respective environments\n",
        "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
        "        \"\"\"\n",
        "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
        "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
        "        \n",
        "        # reset environments automatically\n",
        "        for i in range(len(self.envs)):\n",
        "            if done[i]:\n",
        "                new_obs[i] = self.envs[i].reset()\n",
        "        \n",
        "        return new_obs, rewards, done, infos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JawlpBFo0Yb"
      },
      "source": [
        "env_batch = EnvBatch(10)\n",
        "\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
        "\n",
        "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "\n",
        "print(\"State shape:\", batch_states.shape)\n",
        "print(\"Actions:\", batch_actions[:3])\n",
        "print(\"Rewards:\", batch_rewards[:3])\n",
        "print(\"Done:\", batch_done[:3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwEPCL46pfHO"
      },
      "source": [
        "# Actor-critic\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we define a loss functions and learning algorithms as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C6zD_V1o35d"
      },
      "source": [
        "# These placeholders mean exactly the same as in \"Let's try it out\" section above\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "states_ph = tf.compat.v1.placeholder('float32', [None,] + list(obs_shape))    \n",
        "next_states_ph =  tf.compat.v1.placeholder('float32', [None,] + list(obs_shape))\n",
        "actions_ph =  tf.compat.v1.placeholder('int32', (None,))\n",
        "rewards_ph =  tf.compat.v1.placeholder('float32', (None,))\n",
        "is_done_ph =  tf.compat.v1.placeholder('float32', (None,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktpq6pJgo-Dl"
      },
      "source": [
        "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
        "logits, state_values = agent.symbolic_step(states_ph)\n",
        "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
        "next_state_values = next_state_values * (1 - is_done_ph)\n",
        "\n",
        "# probabilities and log-probabilities for all actions\n",
        "probs = tf.nn.softmax(logits)            # [n_envs, n_actions]\n",
        "logprobs = tf.nn.log_softmax(logits)     # [n_envs, n_actions]\n",
        "\n",
        "# log-probabilities only for agent's chosen actions\n",
        "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aNY8wRpBk9"
      },
      "source": [
        "\n",
        "\n",
        "# compute advantage using rewards_ph, state_values and next_state_values\n",
        "gamma = 0.99\n",
        "advantage = rewards_ph + gamma*next_state_values - state_values\n",
        "\n",
        "assert advantage.shape.ndims == 1, \"please compute advantage for each sample, vector of shape [n_envs,]\"\n",
        "\n",
        "# compute policy entropy given logits_seq. Mind the \"-\" sign!\n",
        "entropy =  -tf.reduce_sum(probs * logprobs, 1, name=\"entropy\")\n",
        "\n",
        "assert entropy.shape.ndims == 1, \"please compute pointwise entropy vector of shape [n_envs,] \"\n",
        "\n",
        "\n",
        "\n",
        "actor_loss =  - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage)) - 0.001 * tf.reduce_mean(entropy)\n",
        "\n",
        "# compute target state values using temporal difference formula. Use rewards_ph and next_step_values\n",
        "target_state_values = rewards_ph+gamma*next_state_values\n",
        "\n",
        "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2 )\n",
        "\n",
        "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
        "sess.run(tf.compat.v1.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9qCcr3cpEtw"
      },
      "source": [
        "# Sanity checks to catch some errors. Specific to KungFuMaster in assignment's default setup.\n",
        "l_act, l_crit, adv, ent = sess.run([actor_loss, critic_loss, advantage, entropy], feed_dict = {\n",
        "        states_ph: batch_states,\n",
        "        actions_ph: batch_actions,\n",
        "        next_states_ph: batch_states,\n",
        "        rewards_ph: batch_rewards,\n",
        "        is_done_ph: batch_done,\n",
        "    })\n",
        "\n",
        "assert abs(l_act) < 100 and abs(l_crit) < 100, \"losses seem abnormally large\"\n",
        "assert 0 <= ent.mean() <= np.log(n_actions), \"impossible entropy value, double-check the formula pls\"\n",
        "if ent.mean() < np.log(n_actions) / 2: print(\"Entropy is too low for untrained agent\")\n",
        "print(\"You just might be fine!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGMW7v42peBa"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFWsGKlpuIM"
      },
      "source": [
        "# Train\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Just the usual - play a bit, compute loss, follow the graidents, repeat a few million times.\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/69cc1e7cffa2ebd5db67c39f7c7c2724aaf5594b/687474703a2f2f696d61676573362e66616e706f702e636f6d2f696d6167652f70686f746f732f33383930303030302f44616e69656c2d73616e2d747261696e696e672d7468652d6b61726174652d6b69642d33383934373336312d3439392d3238382e676966)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6TQ0p8ul1ZA"
      },
      "source": [
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpqugPcIpIlq"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "#from pandas import ewm\n",
        "env_batch = EnvBatch(10)\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "rewards_history = []\n",
        "entropy_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISPHZ1Ed7y-M"
      },
      "source": [
        "x.ewm(halflife=12).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-uNJ4-TpNC_"
      },
      "source": [
        "for i in trange(100000): \n",
        "    \n",
        "    batch_actions = agent.sample_actions(agent.step(batch_states))\n",
        "    batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "    \n",
        "    feed_dict = {\n",
        "        states_ph: batch_states,\n",
        "        actions_ph: batch_actions,\n",
        "        next_states_ph: batch_next_states,\n",
        "        rewards_ph: batch_rewards,\n",
        "        is_done_ph: batch_done,\n",
        "    }\n",
        "    batch_states = batch_next_states\n",
        "    \n",
        "    _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
        "    entropy_history.append(np.mean(ent_t))\n",
        "\n",
        "    if i % 500 == 0: \n",
        "        if i % 2500 == 0:\n",
        "            rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
        "            if rewards_history[-1] >= 50:\n",
        "                print(\"Your agent has earned the yellow belt\")\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=[8,4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(rewards_history, label='rewards')\n",
        "        df = pd.DataFrame(rewards_history)\n",
        "        plt.plot(df.ewm(halflife=1000).mean(), marker='.', label='rewards ewma@10')\n",
        "        plt.title(\"Session rewards\"); plt.grid(); plt.legend()\n",
        "        \n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(entropy_history, label='entropy')\n",
        "        df2 = pd.DataFrame(entropy_history)\n",
        "        plt.plot(df2.ewm(halflife=1000).mean(), label='entropy ewma@1000')\n",
        "        plt.title(\"Policy entropy\"); plt.grid(); plt.legend()        \n",
        "        plt.show()\n",
        "        \n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC3JUqqPpo6k"
      },
      "source": [
        "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
        "final_rewards = evaluate(agent, env_monitor, n_games=1,)\n",
        "env_monitor.close()\n",
        "print(\"Final mean reward:\", np.mean(final_rewards))\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwb5QCk3YUb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
\documentclass[conference]{IEEEtran}
\usepackage[utf8]{inputenc}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{adjustbox}
\graphicspath{ {./Images/} }

\usepackage{biblatex} %Imports biblatex package
\addbibresource{bibliography.bib}
 
\title{
Analysis of The Performance Deep Learning Algorithms In Video Games\\
\vspace{5mm}
\large Department of Computing \\
\vspace{3mm} 
\large Letterkenny Institute of Technology \\
\vspace{3mm} 
\large Artificial Intelligence 2 \\
\vspace{3mm} 
\large Lecturer: Shagufta Henna
}
\author{By Ultan Kearns}
\date{\today}

\begin{document}

\maketitle
\begin{abstract}
    This paper aims to analyze deep learning algorithms and methodologies by utilizing an AI(Artificial Intelligence) Agent to perform specified tasks in simulated environments.  For the purpose of this paper I will be analyzing the Agents performance in video games and will be weighing the pros and cons of each algorithm in respect to the Agent's performance.  Each game has a specific goal for the Agent to complete and the Agent's performance will be evaluated based on certain goals and objectives present in each game discussed in this paper.  I will then analyze the performance of the Agent for each specified environment and determine the strengths and weaknesses of each deep learning methodology used to train the Agent. 
    \\
    \\
    Key Words: AI, Deep Learning, Games
\end{abstract}
\section{Introduction}
\subsection{Purpose of Paper}
The purpose of this paper is to compare and contrast different deep learning methods and algorithms with respect to an AI Agent.  In the upcoming sections I will be documenting the training of the Agent which will be trained to perform certain tasks within a given environment, the environment being the game.  I will evaluate the Agent's performance by reviewing their actions during the game and judging if these actions were desirable or undesirable.  The overall purpose of this paper is to determine how the Agent can maximize it's performance in the given environment and to explore which algorithm will produce the best results for a given environment as well as provide an analysis of said algorithms.
\\
\\
In the upcoming sections I will analyze the performance of each algorithm and investigate if the Agent's performance can be improved using these algorithms by adjusting the hyper-parameters for each model.
\subsection{History of AI Use in Video Games}

\subsection{Deep Learning}
Deep learning is a subset of AI which aims to train an Agent to perform certain tasks.  Deep-learning differs from Machine Learning by using deep layer structures to create an Artificial Neural Network which can make intelligent choices based on a given state in a simulated environment.  There are many methodologies within the field of deep learning which are used to train the agent and in this paper we will analyze the following: Deep Q Learning, Reinforcement Learning \& Advantage Actor Critic.  We will see that each one of these methodologies will have pros \& cons when we evaluate the Agent's performance in each game.
\section{Games}
\subsection{Cart Pole}
\subsubsection{About Game}
In this game the Agent will control a cart which can either move left or right within the screen's boundaries.  The cart has a pole affixed to it which the Agent will try and keep balanced so that it does not pass a certain angle threshold.
\subsubsection{Goals \& Objectives of Agent}
The goal of this game is to move the cart while keeping the pole from falling, the Agent can move either left or right to sustain the pole.  When the pole passes an angle of 15 degree limit the game ends.  The Agent's performance is evaluated by determining how long it can keep the pole from passing the 15 degree limit and ending the game.
\subsection{Kung Fu Master}
\subsubsection{About Game}
This game is a "beat em' up" game in which the Agent can move either left or right with the options of crouching or jumping.  The Agent will also have a health bar which will decrease with each hit they take, when the health bar reaches 0 the Agent will lose a life.  
\subsubsection{Goals \& Objectives of Agent}
The goal of the agent is to defeat all the enemies on-screen and reach the end of the game.  The agent will start off on floor 1 and will work their way up to floor 5, the agent must complete this goal within a time-limit and must aim to maximize it's score by defeating enemies and making it to the end of the level with as much time left as possible.  The Agent will lose the game if their health bar reaches 0 and they are out of lives.
\subsection{Breakout}
\subsubsection{About Game}
In this game the Agent will control a paddle which can move either left or right within a confined space.
\subsubsection{Goals \& Objectives of Agent}
The goal of this game is to hit a ball using the paddle which is controlled by the Agent to break a series of blocks without letting the ball fall beyond the paddle.  The Agent also has a score which it will try to maximize by breaking as many blocks as possible, as the game progresses the Agent's paddle will get smaller and the balls velocity will increase making the game harder for the Agent.  The Agent will have three lives to break all the blocks in the environment, the max possible score in Breakout is 896 which we will use as a metric when judging the Agent's performance.
\section{Algorithms Used for Deep Learning}
\subsection{Deep Q Learning(DQN)}
Traditional Q Learning works by maintaining a table with each state, this method works fine for environments with a small number of states however when working with a higher number of states we opt to use Deep Q Learning. 
\\
\\
DQN differs from traditional Q Learning by using an Artificial Neural Network(ANN) which has the advantage of reducing the amount of memory utilized as DQN learns over time(iterations) and discovers commonalities between states.  
\\
\\
This means that DQN doesn't require having to store every state the agent could be in memory, however the model does take time to train and increasing the number of iterations will increase the time it will take to train.
\\
DQN utilizes an optimal action-value function defined by the following equation:\[Q*(s,a)=max_\pi\epsilon[r_t + \gamma r_t+1 + r_t+2 .....]\]\cite{DQNPaper} 
\begin{itemize}
    \item Where Q is our agent
    \item S is our state
    \item A is the action
    \item r is the reward
    \item t is the time step 
    \item and $\gamma$ is our discount value
\end{itemize}
essentially we are approximating the best course of action for our Agent's given state, in using this function we can approximate the best course of action for our Agent to take given it's current state.
\\
We also use a loss function defined by the following equation for each iteration: \[L_i(\theta_i)=\epsilon_(s,a,r,s')[(r+\gamma*max_a'Q(s',a';\theta_i) - Q(s,a;\theta_i))^2]\]\cite{DQNPaper}
\begin{itemize}
    \item Where L is our loss
    \item i is given iteration
    \item s is our state
    \item r is our reward
    \item $\gamma$ is our discount factor
    \item Q is our agent
    \item $\theta_i$ is our given iteration
\end{itemize}
Using this loss function we calculate the output of our Agent compared to the actual target value so we can determine how well our agent is performing at a given iteration.
\subsection{Asynchronous Advantage Actor-Critic A3C}
The Asynchronous Advantage Actor Critic algorithm utilizes a Deep Neural Network(DNN) to train the Agent to perform certain tasks using Reinforcement Learning\cite{A3CAsyncPaper}. Essentially this algorithm works by implementing an Actor(our Agent) and a Critic which will be used to evaluate the agents performance in the given environment.  It is in this way that the Agent can learn through Reinforcement Learning as the Actor will try to maximize it's score and the critic will use value based functions to determine if the Actor could be performing better.
\\
\\
This algorithm trains multiple agents in parallel on multiple environments each with their own weights, it is in this way that the Agent can experience more states and actions in less time.  The results of these Agents are then combined and used as inputs to train a single Agent or what is termed "Global Network"\cite{A3CAiSummer}.  Using a neural network we can train the Agents to approximate the best state \& the best actions when playing the game.
\\
\\
Our Agent will be trained using a Deep Neural Network so that it can approximate the best action to take for a given state.

\subsection{Reinforcement Learning}

The premise of reinforcement learning is as follows: 
\\
\begin{itemize}
    \item The Agent Performs Task(T) for a given State(S)
    \item The performance of the agent is then evaluated at which point we have two options:
    \begin{enumerate}
        \item Reward the agent if the performance is beneficial to them
        \item "Punish the agent if their performance is undesirable
    \end{enumerate}
\end{itemize}

The way we "reward" or "punish" an agent can be done in a variety of ways usually by utilizing a cost function which will either increase an agents score for a desirable action or punish them for an undesirable one.

\section{Results of Algorithms}
\subsection{DQN For Cartpole}
When using DQN for the CartPole game I found that the agents Average Return plateaued over time for our given hyper-parameters.  When training the agent I first started by seeing if an increase in the number of iterations would have much of an effect on the Average Return.  I started by increasing the number of iterations from 20,000 to 30,000 and assessed the results which I will show below.
\begin{figure}[h]
\caption{Average return for Agent with 20,000 training iterations}
\centering
\includegraphics[width=0.5\textwidth]{Images/DQN20k.png}
\end{figure}
\begin{figure}[H]
\caption{Average return for Agent with 30,000 training iterations}
\centering
\includegraphics[width=0.5\textwidth]{Images/DQN30k.png}
\end{figure}
We can see from the figure above that the performance is not improving much after hitting 20k intervals. 
\begin{figure}[H]
\caption{Average return for Agent with 50,000 training iterations}
\centering
\includegraphics[width=0.5\textwidth]{Images/DQN50k.png}
\end{figure}
I also decided to see what effect training for 50k iterations would have, as we can see from the image below the performance actually decreases when after a certain number of iterations has been reached.  
\subsubsection{Hyper Parameters Used}
The above results were achieved using the following hyper parameters:
\begin{itemize}
    \item Number of iterations: 20,000
    \item Initial Random Steps: 1,000
    \item Steps per Iteration: 1
    \item Data Storage for Training: 100000
    \item Batch Size: 64
    \item Learning Rate: 1-e3
    \item Log Interval: 200
    \item Evaluation Episodes: 10
    \item Evaluation Interval: 1000
\end{itemize}
\subsubsection{Adjusting Hyper-Parameters}
\subsubsection{Final Results}

\subsection{Actor Action Critic(A3C)}

\subsubsection{Adjusting Hyper-Parameters}

\subsection{Reinforcement-Learning Breakout}
\subsubsection{Adjusting Hyper-Parameters}
\subsubsection{Final Results}

\subsection{Reinforcement-Learning Breakout}
\subsubsection{Adjusting Hyper-Parameters}
\subsubsection{Final Results}

\subsection{Pros \& Cons}
\subsubsection{DQN}
\subsubsection{A3C}
\subsubsection{Reinforcment Learning}
\section{Conclusion}

\newpage
\printbibliography
\end{document}
